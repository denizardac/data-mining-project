# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fk8QEohTHCcGwVUoGwRytYntyPZ2qcvr
"""

import pandas as pd
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, matthews_corrcoef
from imblearn.combine import SMOTEENN
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict  # cross_val_predict'i import ettik
import numpy as np
import warnings
warnings.filterwarnings("ignore")

# Veri Yükleme
file_path = "Data_processed.xlsx"  # Excel dosyasının yolu
data = pd.read_excel(file_path)

# Hedef sütunu karaktere çevirerek özel değerleri kontrol edin
data['Longitude'] = data['Longitude'].astype(str)

# Sorunlu değerleri belirli bir formata çevirmek
data['Longitude'] = data['Longitude'].str.replace(r'85\.1\s*4', '85.14', regex=True)

# Sayısala dönüştürme
data['Longitude'] = pd.to_numeric(data['Longitude'], errors='coerce')

# 1. Imputation (Eksik Verileri Doldurma)
numeric_data = data.select_dtypes(include=['float64', 'int64'])
categorical_data = data.select_dtypes(include=['object'])
# Eksik verileri içeren sütunları tespit et
missing_values_before = numeric_data.isnull().sum()
numeric_imputer = SimpleImputer(strategy='mean')
data[numeric_data.columns] = numeric_imputer.fit_transform(numeric_data)


categorical_imputer = SimpleImputer(strategy='most_frequent')
data[categorical_data.columns] = categorical_imputer.fit_transform(categorical_data)

pip install catboost

from catboost import CatBoostClassifier
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder

# Verileri hazırlama (örneğin, feature ve target değişkeni)
# 'State_*' ve 'LandType_*' sütunlarını seçmek
state_columns = [col for col in data.columns if col.startswith('State_')]
landtype_columns = [col for col in data.columns if col.startswith('LandType_')]
selected_columns = state_columns + landtype_columns

# Hedef değişkeni kategoriden sayıya dönüştürme
data['GrainYield'] = data['GrainYield'].map({'C': 0, 'B': 1, 'A': 2})

comparison_results = {}  # Sonuçları saklamak için bir sözlük

# Modeller ve çapraz doğrulama
for group, group_columns in zip(["State", "LandType"], [states, landtypes]):
    for column in group_columns:
        print(f"\nAnalyzing group: {column}")

        # Veriyi filtrele
        filtered_data = data[data[column] == 1]
        print(f"Filtered data shape for group '{column}': {filtered_data.shape}")

        # Boş veya tek sınıf kontrolü
        if filtered_data.empty or filtered_data['GrainYield'].nunique() <= 1:
            print(f"Skipping group: {column} due to insufficient or single-class data")
            continue

        # Özellik ve hedef değişken tanımlama
        X = filtered_data.drop(columns=["GrainYield"] + selected_columns, errors='ignore')
        y = filtered_data['GrainYield']

        # SMOTE kullanarak dengeleme
        print("Class distribution before SMOTE:", y.value_counts())
        smote = SMOTE(random_state=42)
        try:
            X_res, y_res = smote.fit_resample(X, y)
        except ValueError as e:
            print(f"Skipping group: {column} due to SMOTE error: {e}")
            continue

        print("Class distribution after SMOTE:", np.unique(y_res, return_counts=True))

        # Modeller ve çapraz doğrulama
        models = {
            "Random Forest": RandomForestClassifier(class_weight='balanced', n_estimators=200, max_depth=10, random_state=42),
            "Logistic Regression": LogisticRegression(solver='liblinear', class_weight='balanced', C=1.0),
            "SVM": SVC(probability=True, class_weight='balanced', C=1.0, gamma='scale'),
            "KNN": KNeighborsClassifier(n_neighbors=5),
            "Naive Bayes": GaussianNB(),
            "Decision Tree": DecisionTreeClassifier(class_weight='balanced', max_depth=10, random_state=42),
            "Gradient Boosting": GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42),
            "AdaBoost": AdaBoostClassifier(n_estimators=200, random_state=42),
            "Bagging": BaggingClassifier(n_estimators=50, random_state=42),
            "CatBoost": CatBoostClassifier(iterations=200, learning_rate=0.1, depth=5, random_state=42, cat_features=[])  # CatBoost Model
        }

        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        group_results = {}

        for name, model in models.items():
            try:
                cv_results = cross_val_score(model, X_res, y_res, cv=cv, scoring='accuracy')
                group_results[name] = {
                    "Accuracy": cv_results.mean(),
                    "Std": cv_results.std()
                }
            except Exception as e:
                group_results[name] = {"Error": str(e)}

        comparison_results[column] = group_results

        # Sonuçları yazdır
        print(f"Results for column: {column}")
        for model_name, metrics in group_results.items():
            if "Error" not in metrics:
                print(f"  {model_name}: Accuracy = {metrics['Accuracy']:.4f}, Std = {metrics['Std']:.4f}")
            else:
                print(f"  {model_name}: Error = {metrics['Error']}")



# 'State_*' ve 'LandType_*' Sütunlarını Seçmek
from sklearn.feature_selection import RFE
state_columns = [col for col in data.columns if col.startswith('State_')]
landtype_columns = [col for col in data.columns if col.startswith('LandType_')]
selected_columns = state_columns + landtype_columns

# Seçilen özellikleri saklayacağımız sözlük
selected_features = {}

# Her bir state ve landtype için RFE uygulayacağız
for column in state_columns + landtype_columns:
    print(f"Feature Selection for: {column}")

    # Hedef değişkeni (GrainYield) encode etme
    y_encoder = LabelEncoder()
    y = y_encoder.fit_transform(data['GrainYield'])
    X = data.drop(['GrainYield'], axis=1)  # GrainYield dışındaki tüm sütunlar

    # SMOTE-ENN ile dengeleme
    smoteenn = SMOTEENN(random_state=42)
    X_res, y_res = smoteenn.fit_resample(X, y)

    # RFE ile özellik seçimi
    model = RandomForestClassifier(random_state=42)
    rfe = RFE(model, n_features_to_select=10)
    rfe.fit(X_res, y_res)

    # Seçilen özellikleri saklama
    selected_features[column] = list(X.columns[rfe.support_])
    print(f"Selected Features for {column}: {selected_features[column]}")

from sklearn.metrics import precision_recall_fscore_support
# Test ve Değerlendirme
metrics_results = {}
for name, model in models.items():
    model.fit(X_res, y_res)
    y_pred = model.predict(X_res)
    precision, recall, f1, _ = precision_recall_fscore_support(y_res, y_pred, average='weighted')
    mcc = matthews_corrcoef(y_res, y_pred)
    metrics_results[name] = {
        "Precision": precision,
        "Recall": recall,
        "F1": f1,
        "MCC": mcc
    }

# RFE ile özellik seçimi
model = RandomForestClassifier(random_state=42)
rfe = RFE(model, n_features_to_select=10)
rfe.fit(X_res, y_res)

# Seçilen özelliklerin listesi
selected_features_rfecv = X_res.columns[rfe.support_]
print("RFE ile seçilen sütunlar:", selected_features_rfecv.tolist())

# Eksik sütunları kontrol etme
missing_features = [col for col in selected_features_rfecv if col not in X_res.columns]
if missing_features:
    print("Eksik sütunlar:", missing_features)

# Seçilen özelliklere göre yeni X matrisi oluşturma
X_selected_features = X_res[selected_features_rfecv]

from sklearn.model_selection import cross_val_score, StratifiedKFold
import pandas as pd

# Metrikleri hesaplayacak fonksiyon
def get_metrics(model, X, y, cv):
    try:
        accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()
        auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()
        f1 = cross_val_score(model, X, y, cv=cv, scoring='f1_macro').mean()
        precision = cross_val_score(model, X, y, cv=cv, scoring='precision_macro').mean()
        recall = cross_val_score(model, X, y, cv=cv, scoring='recall_macro').mean()
        mcc = cross_val_score(model, X, y, cv=cv, scoring='balanced_accuracy').mean()

        # Eğer AUC 'nan' ise, bunu 0'a eşitle
        if auc != auc:  # NaN kontrolü
            auc = 0.0

        return {
            "CA": accuracy,
            "AUC": auc,
            "F1": f1,
            "Precision": precision,
            "Recall": recall,
            "MCC": mcc
        }
    except Exception as e:
        print(f"Error calculating metrics: {e}")
        return None

# Performans karşılaştırma tablosu
def compare_metrics(metrics_no_fs, metrics_fs):
    df_no_fs = pd.DataFrame(metrics_no_fs).T
    df_fs = pd.DataFrame(metrics_fs).T
    comparison = pd.concat([df_no_fs, df_fs], axis=1, keys=['No FS', 'FS'])
    return comparison

# Sınıf dengesizliği ve küçük veri setleri gibi durumlar için kontrol ekledik.
metrics_no_fs = {}
metrics_fs = {}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
best_model_name = None
best_model_score = -1
best_model = None

for name, model in models.items():
    try:
        # Feature Selection olmadan
        metrics_no_fs[name] = get_metrics(model, X_res, y_res, cv)

        # Feature Selection ile
        metrics_fs[name] = get_metrics(model, X_selected_features, y_res, cv)

        accuracy_no_fs = metrics_no_fs[name]["CA"]
        accuracy_fs = metrics_fs[name]["CA"]
        print(f"{name} - Accuracy (No FS): {accuracy_no_fs:.4f}, Accuracy (FS): {accuracy_fs:.4f}")

        # En iyi modeli Accuracy'ye göre seç
        if metrics_fs[name]["CA"] > best_model_score:
            best_model_score = metrics_fs[name]["CA"]
            best_model_name = name
            best_model = model

    except Exception as e:
        print(f"{name} - Error: {e}")

# Performans karşılaştırma ve yazdırma
result_df = compare_metrics(metrics_no_fs, metrics_fs)
print(result_df)

# En iyi modelin bilgisi
if best_model:
    print(f"\nBest Model (based on Accuracy): {best_model_name} with Accuracy = {best_model_score:.4f}")
else:
    print("No best model found.")

# NaN içeren satırları kontrol et
nan_rows = result_df[result_df.isna().any(axis=1)]
display(nan_rows)

from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import cross_val_predict
import matplotlib.pyplot as plt
import numpy as np

# ROC Eğrisi çizimi
plt.figure(figsize=(10, 8))

for name, model in models.items():
    try:
        # Çapraz doğrulama ile tahminleri alıyoruz
        y_pred_prob = cross_val_predict(model, X_res, y_res, cv=cv, method='predict_proba', n_jobs=-1)

        # Eğer binary sınıflandırma ise
        if len(np.unique(y_res)) == 2:
            fpr, tpr, _ = roc_curve(y_res, y_pred_prob[:, 1])
            roc_auc = auc(fpr, tpr)
            plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')

        # Eğer çok sınıflı sınıflandırma ise OvR yaklaşımlarını birleştir
        else:
            # OvR (One-vs-Rest) için sınıfları gez
            fpr = dict()
            tpr = dict()
            roc_auc = dict()
            for i in range(len(np.unique(y_res))):
                fpr[i], tpr[i], _ = roc_curve(y_res == i, y_pred_prob[:, i])
                roc_auc[i] = auc(fpr[i], tpr[i])

            # Ortalama ROC eğrisi
            all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(np.unique(y_res)))]))
            mean_tpr = np.zeros_like(all_fpr)
            for i in range(len(np.unique(y_res))):
                mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
            mean_tpr /= len(np.unique(y_res))
            mean_auc = auc(all_fpr, mean_tpr)

            plt.plot(all_fpr, mean_tpr, lw=2, label=f'{name} (OvR AUC = {mean_auc:.2f})')

    except Exception as e:
        print(f"Error plotting ROC for {name}: {e}")

# Grafik ayarları
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for All Models (Combined)')
plt.legend(loc='lower right')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import cross_val_predict, StratifiedKFold
import numpy as np

# En iyi modelin tanımlandığını ve seçildiğini varsayıyoruz (best_model ve best_model_name mevcut)
if best_model:
    # Çapraz doğrulama tahminlerini alın
    y_pred_best = cross_val_predict(best_model, X_selected_features, y_res, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42))

    # Confusion matrix oluştur
    cm = confusion_matrix(y_res, y_pred_best)

    # Confusion matrix görselleştirme
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_res), yticklabels=np.unique(y_res))
    plt.title(f'Confusion Matrix - {best_model_name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    # Ek metrikleri yazdır
    print(f"Classification Report for {best_model_name}:\n")
    print(classification_report(y_res, y_pred_best))
else:
    print("No best model found.")
